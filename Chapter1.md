**Chapter 1. AWS 기반 데이터 과학 소개**<br>
-	Amazon.com Recommendations: Items-to-Item Collaborative Filtering논문
-	클라우드는 데이터과학 프로젝트를 개발하는 것은 물리적으로 자체 서버 인프라를 구축하지 않고서도 프로토타입에서 프로덕션으로 모델을 원활하게 전화할 수 있게 해준다.
  
1.2.3.	큐브플로우 파이프라인: 쿠버네티스 환경에 빌드된 Kubeflow pipeline이라는 오케스트레이션 하위 시스템을 포함하는 비교적 새로운 오픈 소스 머신러닝 플랫폼이다.
 	
1.2.4.	Apache Airflow는 주로 데이터 엔지니어링과 추출extract_변환 Transformaion_로드 load(ETL) 파이프라인을 조정하는 인기있는 오픈소스 워크플로우 플랫폼이다. <br>
A pipeline orchestrator is a tool that helps to automate these workflows. An orchestrator can schedule jobs, execute workflows, and coordinate dependencies among tasks.

1.3.3. 신뢰성: 우리는 훈련 데이터에 대한 변경 사항 트래킹 및 버전 관리를 자동화해야 한다. 이를 통해 오류가 발생해도 정확하게 이전 버전의 모델로 다시 되돌릴 수 있다.

1.5.1 아마존 S3와 AWS 레이크 포메이션을 활용한 데이터 수집과 데이터 레이크<br>
- 데이터에 엑세스하고 분석하는 팀과 애플리케이션이 점점 더 많아지면서 많은 기업에서는 확장성이 뛰어나고 가용성이 높으며 안전하고 유연한 데이터 저장소인 데이터 레이크 data lake로 이동하고 있다. 

1. 세이지메이커는 커스텀 코드와 S3로부터 복사한 데이터와 도커Docker 테이너를 이용하여 프로세싱 작업을 실행한다.<br>
1) Sage Maker: 데이터 처리와 머신러닝 모델을 훈련하는 데 필요한 모든 작업을 쉽게 할 수 있도록 도와주는 클라우드 기반 도구 (AWS(Amazon Web Services)에서 제공하는 머신러닝(ML) 서비스)<br>
2) Custom Code: 사용자가 자신만의 방법으로 데이터를 처리하거나 모델을 훈련시키기 위해 작성한 **프로그래밍 코드**<br>
3) S3: 아마존의 클라우드 저장소입니다. 여기에는 다양한 데이터가 저장될 수 있는데, 예를 들어 CSV 파일, 이미지, 텍스트 등 여러 형태의 데이터가 있을 수 있습니다.<br>
4) **도커(Docker)**:는 소프트웨어를 실행하기 위한 가상 환경을 만들 수 있는 도구입니다. 세이지메이커에서 "도커 컨테이너"는 작업을 실행하는 환경이라고 생각할 수 있습니다.<br>
5) 프로세싱: 프로세싱 작업은 데이터 분석이나 처리, 모델 훈련 등을 의미합니다. 세이지메이커에서 "프로세싱"이란, 데이터를 읽고 처리하거나, 학습된 모델을 사용하여 예측하는 등의 작업을 뜻합니다.

쉽게 말해서: 세이지메이커는 클라우드에서 데이터를 처리하고 머신러닝 모델을 학습시키기 위해 필요한 모든 작업을 할 수 있게 해주는 도구입니다. 그리고 이 과정은 데이터를 가져오는 것, 코드를 실행하는 것, 환경을 설정하는 것이 모두 자동으로 연결되어 이루어집니다.

**비유**: 마치 셰프가 주방에서 요리를 만드는 과정과 비슷합니다.

- S3는 재료 보관소: 주방에 가기 전에 필요한 재료를 S3에서 가져옵니다. <br>
- 커스텀 코드는 요리법: 셰프가 자신만의 레시피대로 요리합니다. <br>
- 도커는 주방: 요리하는 데 필요한 모든 도구와 환경이 준비된 주방입니다. 셰프는 여기에서 작업을 안전하고 효율적으로 할 수 있습니다.

1.6.4 자체 컨테이너 작성하기 <br>
빌트인 알고리즘과 스크립트 모드를 둘 다 사용할 수 없는 경우 자체 커스텀 도커 이미지를 가져와서 모델 훈련을 실행할 수 있다. 참고로 도커는 도커 컨테이너 Docker container라는 격리된 환경의 빌드타임 build-time 및 런타임 지원을 제공하는 소프트웨어 도구다. 

1.7 아마존 세이지메이커와 AWS 람다 함수를 사용한 모델 배포 <br>
모델을 훈련, 검증 및 최적화를 마치고 나면 모델을 배포하고 모니터링할 단계이다.어플리케이션 요구 사항에 따라 아마존 세이지메이커를 사용해 모델을 배포하는 방법으로 REST 기반 예측을 위한 세이지메이커 엔드포인트, 서버리스 예측을 위한 AWS 람다 함수, 일괄 예측을 위한 세이지메이커 일괄 변환 batch transform등을 아용한다. 

1.7.1 세이지 메이커 엔드포인트 <br>
만약 모델 배포에 있어서 지연 시간을 낮추고 실시간 예측을 위해 모델 배포 환경을 최적화해야 한다면 세이지메이커 호스팅 서비스를 사용하면 된다. 세이지메이커 호스팅 서비스는 모델을 소트하기 위한 세이지메이커 엔드포인트 서비스와 이를 통해 모델 추론을 수행할 수 있는 REST API를 제공한다. 세이지메이커 모델 엔드포인트는 실시간 트래픽 패턴에 맞춰 자동으로 클라우드 리소스 스케일을 조정하고, 높은 가용성을 위해 여러 가용 영역에 배포된다.

1.8.2 **Apache Kafka**는 복잡한 시스템에서 **실시간으로 데이터를 처리하고 전송**하는 데 사용되는 도구입니다. 

### 1. **Kafka란 무엇인가?**

Apache Kafka는 **실시간 데이터 스트리밍 플랫폼**입니다. 이를 통해 다양한 시스템들이 **데이터를 실시간으로 주고받을 수 있게** 해줍니다. 예를 들어, 웹사이트에 방문한 사람들이 남긴 댓글이나 트위터에서 발생한 메시지처럼, 빠르게 변화하는 데이터를 **실시간으로 처리**하고 **다른 시스템에 전달**하는 데 사용됩니다.

### 2. **간단한 비유: Kafka는 '우편배달 시스템'과 비슷하다**

Kafka는 **우편배달 시스템**과 비슷하다고 생각할 수 있습니다. 여러 사람들이 각각 다른 장소에 우편물을 보내고, 그 우편물은 우체국에 모입니다. 그런 다음 우체국에서 다른 사람이나 조직으로 우편물이 전달됩니다.

#### 예시:

- **우편물**: 실시간 데이터, 예를 들어 웹사이트에서 사람들이 클릭한 정보, 센서가 보내는 데이터, 소셜 미디어에서 발생하는 글들 등.
- **우체국**: Kafka의 **중앙 시스템**입니다. 데이터를 모으고, 필요한 곳으로 전달하는 역할을 합니다.
- **우체국에서 보내는 사람**: Kafka의 **프로듀서(producer)**입니다. 데이터(우편물)를 Kafka에 보내는 시스템입니다.
- **우체국에서 받는 사람**: Kafka의 **컨슈머(consumer)**입니다. 데이터를 받는 시스템으로, 데이터가 필요한 곳으로 전달됩니다.

### 3. **Kafka의 주요 구성 요소**

Kafka는 세 가지 주요 요소로 이루어져 있습니다:

- **Producer(생산자)**: 데이터를 보내는 시스템입니다. 예를 들어, 웹사이트에서 클릭한 데이터를 Kafka로 보내는 프로그램.
- **Broker(중개자)**: Kafka가 데이터를 모아서 저장하고 전달하는 역할을 합니다. 마치 우체국처럼 데이터를 보관하고, 필요할 때 전달합니다.
- **Consumer(소비자)**: Kafka에서 데이터를 가져가서 처리하는 시스템입니다. 예를 들어, 클릭된 데이터를 분석하는 시스템.

### 4. **Kafka가 왜 중요한가?**

Kafka는 **대규모로 실시간 데이터를 처리**할 수 있기 때문에, 대규모 시스템이나 **실시간 서비스**에서 매우 유용합니다. 예를 들어:

- **웹사이트**에서 수많은 사람들이 동시에 클릭을 하고, 그 데이터를 실시간으로 다른 시스템에 전달해야 할 때.
- **모바일 앱**에서 사용자들의 행동 데이터를 실시간으로 분석해야 할 때.
- **금융 거래 시스템**에서 거래가 발생하는 즉시 데이터를 다른 시스템에 전달해야 할 때.

### 5. **Kafka의 특징**

1. **고속 데이터 처리**: Kafka는 실시간으로 **빠른 속도로 데이터를 처리**할 수 있습니다. 수백만 개의 메시지를 초당 처리할 수 있습니다.
2. **내구성**: Kafka는 데이터를 **안전하게 저장**합니다. 만약 시스템에 문제가 생기더라도 데이터를 잃어버리지 않고, 다시 복구할 수 있습니다.
3. **확장성**: Kafka는 데이터의 양이 많아져도, 시스템을 **확장하여 처리할 수** 있습니다. 즉, 수백 대의 서버에서 동시에 데이터를 처리할 수 있습니다.
4. **다양한 용도**: Kafka는 데이터를 **실시간으로 스트리밍**하거나 **저장**하는 용도로 사용할 수 있습니다. 데이터 분석, 실시간 모니터링, 이벤트 처리 등 다양한 용도로 활용됩니다.

### 6. **Kafka 사용 예시**

- **실시간 분석**: 예를 들어, 쇼핑몰에서 사람들이 어떤 상품을 클릭하는지, 구매하는지를 실시간으로 분석하여 그에 맞는 광고를 보여줄 수 있습니다.
- **모니터링 시스템**: 서버의 상태나 웹사이트 트래픽 데이터를 실시간으로 모니터링하고, 문제가 발생하면 즉시 알림을 보낼 수 있습니다.
- **데이터 파이프라인**: 여러 시스템이 데이터를 주고받을 때 Kafka를 사용하면, 실시간으로 데이터를 다른 시스템에 전달하고 처리할 수 있습니다.

### 7. **간단한 요약**

Apache Kafka는 **실시간 데이터 스트리밍 플랫폼**으로, 데이터를 빠르고 안전하게 전송하고 처리하는 데 사용됩니다. 여러 시스템 간에 데이터를 **실시간으로 주고받을 수 있게** 해주는 중요한 역할을 하며, 특히 **대규모 시스템**이나 **빠르게 변화하는 데이터**를 다룰 때 유용합니다.

### 핵심 포인트

- **Kafka**는 **실시간 데이터 전송 시스템**.
- **Producer**는 데이터를 보내는 사람, **Consumer**는 데이터를 받는 사람.
- **Broker**는 데이터를 모아서 전달하는 시스템.
- **우체국**처럼 데이터를 안전하게 관리하고 필요한 곳에 전달.

이렇게 Kafka는 복잡한 시스템에서 발생하는 **실시간 데이터**를 **효율적으로 처리**하고 전달하는 데 도움을 줍니다.



